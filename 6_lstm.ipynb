{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: {\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2id('{')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "# Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295242 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "namtcpjrafowntaielgxrslcuokda anymolyih wydloaqrpng   eiw  ghdcc hyx idrek fe nq\n",
      "jeohbt tjo iuztfauirbaso   hltiz  w hatyuatdcyab hd agezkdl lwtufbsxbdd   ntnh i\n",
      "obetb ydtibhvmzme jvogpb bgwd dvjujz wfl id anelih olkvq  mklmde t kymxrdisaaeuu\n",
      "fdeseicax  efawrhmnmlropush jtjl qpeoexvefri tpueuvut eho hpsmseyatddghqqqrrkm c\n",
      "qrpahaies n  ms g alph huxdorvnaeklii xfljr d oolb xll irmzruih yawa aom gmbimkc\n",
      "================================================================================\n",
      "Validation set perplexity: 20.31\n",
      "Average loss at step 100: 2.591028 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.00\n",
      "Validation set perplexity: 10.55\n",
      "Average loss at step 200: 2.257372 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.60\n",
      "Validation set perplexity: 8.56\n",
      "Average loss at step 300: 2.100901 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.38\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 400: 2.002235 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 500: 1.936285 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 600: 1.911929 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 700: 1.859643 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 800: 1.819303 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 900: 1.828848 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 1000: 1.824320 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "fine cepter foul lowe almoders is dief the religity trees of this kellored clyed\n",
      "ples of the sing pollions tores morpencessidutter exconderce from confide livy s\n",
      "uus of nemegyxate syscion a canilister couve as as or the and meceas viscuder fo\n",
      "verim free learl interyic icaess in hadun oe the ecentley reliled buoped of fore\n",
      "colo howhes by communtions anbirolt aupobrianm in puuphed assectily refiricanici\n",
      "================================================================================\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1100: 1.775710 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1200: 1.751906 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1300: 1.729861 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1400: 1.742863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1500: 1.738465 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1600: 1.745254 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1700: 1.711473 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1800: 1.678254 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 1900: 1.652066 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2000: 1.696515 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "================================================================================\n",
      "nety worllai f of the styst unjodly one nine was funtte chirepon the strock wad \n",
      "en scuding of a senfiie destine fail and is beadized ben c reter equerin p intin\n",
      "ing sandoom dyndin and respex by the recond was a actrics instetsomplinaity fer \n",
      "binenity center was cityock ding on mudily as midaar stacking d one five unjod b\n",
      "rys and nost by no to noinsing such the sastris posist in whis wowndal vainse ca\n",
      "================================================================================\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2100: 1.683884 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2200: 1.688315 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 2300: 1.642275 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2400: 1.658233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2500: 1.678608 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2600: 1.654477 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 2700: 1.660215 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2800: 1.651351 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 2900: 1.651284 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3000: 1.650574 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      " anso hm the aartard in repaced mays and heaption eight four six five five pupli\n",
      " to ioples herse bortze invility dayel is japbain and noability the campoired tr\n",
      "ite and his official sinnelegranced to restrationed madeling and companation opa\n",
      "elepter loz of this revide in the cateras with s aprod s victsounth ascheal gece\n",
      "wake roms soqueram is with in gorse a lovori active lateralizens ragically one s\n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3100: 1.627576 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3200: 1.647387 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3300: 1.633563 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3400: 1.668939 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3500: 1.656984 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3600: 1.670991 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3700: 1.645640 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3800: 1.643360 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3900: 1.635308 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4000: 1.654011 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "boya more then the down tammanted gazyle and breas computer sther idey and novel\n",
      "ing wan meduced rih miciments so only reforticzs team the wajres phitap age hes \n",
      "w forued the bull chicke the mind compiter of four kmed this all to contry cee m\n",
      "x one zero zero flysh ho bow en the collistriolation seeq ach cite not five file\n",
      "qued part rerous eloged reclesive to four equess man beornomal germangzscheppes \n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4100: 1.631890 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4200: 1.639842 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4300: 1.616194 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4400: 1.612684 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 4500: 1.618598 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4600: 1.619252 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4700: 1.629882 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4800: 1.633345 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4900: 1.634493 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5000: 1.604791 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "k the end old useft births rn balptise may of the onited joint has often longnog\n",
      "zer jower would of the to the due s vix treasa corchablin lemo sassaboul peycion\n",
      "inger appean to ractered point dissus even the envancus consist existerulization\n",
      "ull he fact limatly in tabuled s amon toll feat hupe and caxing the latk is et p\n",
      "al peastt hirrologna kinding comp files birts as juna to beacks certairs in reca\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5100: 1.609687 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5200: 1.595631 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5300: 1.584176 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5400: 1.581965 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5500: 1.574855 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5600: 1.582266 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5700: 1.574136 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5800: 1.586306 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5900: 1.575851 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6000: 1.544742 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "al himfelldg others between nooplankeny varya play their stollas a po the reada \n",
      "e day at not the bills alstroceder sclaising the ground and one nine nine three \n",
      "zazish nepits katef crastople and which was a calline had mother zero represser \n",
      "kern kany many moached that way shart because the fam when spilard froncess five\n",
      "zer play statems fubbitary nat hepprim marist frum ploted to dang and of are di \n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6100: 1.569706 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6200: 1.545747 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6300: 1.551879 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6400: 1.544879 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6500: 1.562383 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6600: 1.599912 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6700: 1.581932 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6800: 1.604689 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6900: 1.590552 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 7000: 1.579345 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "quire when lavy of bria buduence minis combud in the webserfaintary day blieic r\n",
      "quantth one nine two complesional christivally re dased princated verous valuage\n",
      "fornist jelitanky energe and as mi such memean centre be same the sections can w\n",
      "proul out two ppicaration can japake be the gromm are with s proceded beamics of\n",
      "n or all due a compared at the harring he ass in abmessage sional come prenaling\n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n",
      "CPU times: user 6min 34s, sys: 46.1 s, total: 7min 20s\n",
      "Wall time: 4min 8s\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal(\n",
    "        [vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal(\n",
    "        [vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.\n",
    "    cx = tf.Variable(tf.truncated_normal(\n",
    "        [vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal(\n",
    "        [vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal(\n",
    "        [num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    # labels are inputs shifted by one time step.\n",
    "    train_labels = train_data[1:]\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "\n",
    "% % time\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few\n",
    "            # batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval(\n",
    "                            {sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable/read:0\", shape=(27, 256), dtype=float32) Tensor(\"Variable_1/read:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable_2/read:0\", shape=(1, 256), dtype=float32)\n",
      "Tensor(\"add_4:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable/read:0\", shape=(27, 256), dtype=float32) Tensor(\"Variable_1/read:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable_2/read:0\", shape=(1, 256), dtype=float32)\n",
      "Tensor(\"add_7:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable/read:0\", shape=(27, 256), dtype=float32) Tensor(\"Variable_1/read:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable_2/read:0\", shape=(1, 256), dtype=float32)\n",
      "Tensor(\"add_10:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable/read:0\", shape=(27, 256), dtype=float32) Tensor(\"Variable_1/read:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable_2/read:0\", shape=(1, 256), dtype=float32)\n",
      "Tensor(\"add_13:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable/read:0\", shape=(27, 256), dtype=float32) Tensor(\"Variable_1/read:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable_2/read:0\", shape=(1, 256), dtype=float32)\n",
      "Tensor(\"add_16:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable/read:0\", shape=(27, 256), dtype=float32) Tensor(\"Variable_1/read:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable_2/read:0\", shape=(1, 256), dtype=float32)\n",
      "Tensor(\"add_19:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable/read:0\", shape=(27, 256), dtype=float32) Tensor(\"Variable_1/read:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable_2/read:0\", shape=(1, 256), dtype=float32)\n",
      "Tensor(\"add_22:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable/read:0\", shape=(27, 256), dtype=float32) Tensor(\"Variable_1/read:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable_2/read:0\", shape=(1, 256), dtype=float32)\n",
      "Tensor(\"add_25:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable/read:0\", shape=(27, 256), dtype=float32) Tensor(\"Variable_1/read:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable_2/read:0\", shape=(1, 256), dtype=float32)\n",
      "Tensor(\"add_28:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable/read:0\", shape=(27, 256), dtype=float32) Tensor(\"Variable_1/read:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable_2/read:0\", shape=(1, 256), dtype=float32)\n",
      "Tensor(\"add_31:0\", shape=(1, 256), dtype=float32) Tensor(\"Variable/read:0\", shape=(27, 256), dtype=float32) Tensor(\"Variable_1/read:0\", shape=(64, 256), dtype=float32) Tensor(\"Variable_2/read:0\", shape=(1, 256), dtype=float32)\n",
      "Initialized\n",
      "Average loss at step 0: 3.297447 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "qiu gew   uxa dieoex p jv seenijetjriaeq ap yoqrr djum rvf agsy  izadi pvdmreeoa\n",
      "ybrr suosfenedyorraee n etb wyrm  munf qecoezvoaye kmfo w uun jn  ytfb   tatzrkd\n",
      "di  bidn aaicyerkuxftsroz etz    rbadvkku  gsxisdyoal snzxdeiknr dac e tj ijn jy\n",
      "cj eqgevsfct slexcta mgpseurelibbbms eyfgpxes nlnltxbhzchnkmoqnfr ed  tl o i  fg\n",
      "a rswe  t eryeq  niiimssxc c gfo q rtfsh eimhqm qikays wua asxotsgcly oocbaqg fs\n",
      "================================================================================\n",
      "Validation set perplexity: 20.28\n",
      "Average loss at step 100: 2.590436 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.29\n",
      "Validation set perplexity: 10.57\n",
      "Average loss at step 200: 2.262918 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.76\n",
      "Validation set perplexity: 8.86\n",
      "Average loss at step 300: 2.122085 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.53\n",
      "Validation set perplexity: 8.38\n",
      "Average loss at step 400: 2.027730 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.12\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 500: 1.969254 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 600: 1.906856 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 700: 1.884306 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 800: 1.835204 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 900: 1.818162 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 1000: 1.799956 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "================================================================================\n",
      "di the eight that corsegoing polisicy it quects his gord the s deviod for held w\n",
      "ic undiver uning thic zero are of sirectution of the mfter the face erecver witl\n",
      " uf inter angern poline two niger s dirron a k one newers one nine five two zero\n",
      "zer his pree predolan rimce convels histauge of tro two were one nine seven ince\n",
      "h in the y as boccomus wonk the callory four doming consailt butth andy idenes d\n",
      "================================================================================\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 1100: 1.783119 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1200: 1.766998 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 1300: 1.744401 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1400: 1.736689 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1500: 1.756528 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1600: 1.742167 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1700: 1.714369 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1800: 1.738015 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1900: 1.728700 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2000: 1.694759 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "================================================================================\n",
      "k chame the relizigutionulishernest an taises that liand lifent cursestly aignde\n",
      "k user mole derucics all in two five one the surnicess form to ressarted to the \n",
      "ver for probory rasksherg notter by constrice i vymber of eight quarthmned by co\n",
      "nages after evipated in the herwerwen acresesly the esgerich pluma of alumic is \n",
      "onoxing hings may but frantaly one nine five five seven the taculery referress o\n",
      "================================================================================\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2100: 1.685025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2200: 1.662280 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2300: 1.700883 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2400: 1.684252 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2500: 1.662001 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2600: 1.640470 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2700: 1.649676 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2800: 1.656911 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2900: 1.628817 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 3000: 1.626288 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "================================================================================\n",
      "ght such of waily he women differual typolution i the factial of the tope placed\n",
      "vely charted firthoginater durable four four fremple sulbarake iz termmes clanse\n",
      "hat riter with the the also be explemon franky when contrencifulion it metoay co\n",
      "tic ictabl matten relt blay parts contince mailsowing apqually ff meetited she b\n",
      "mer the uslain compirain some have exponurately allimation but fartails offen in\n",
      "================================================================================\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3100: 1.654596 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3200: 1.649849 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3300: 1.630965 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3400: 1.624209 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3500: 1.620951 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3600: 1.592015 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3700: 1.606788 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3800: 1.615489 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3900: 1.630259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4000: 1.606634 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "================================================================================\n",
      "x of the nine by in the supe cancessity tibeomin them believelo d one nine withl\n",
      "ed that germanus two zero zero zero fortstique pom varial gave two two hournalia\n",
      "pa harvovel auvo das had as belietigntuly athantion zero sarks down the genen ri\n",
      "plandance was that thid fancext multion and le bome leansense is to made hind ov\n",
      "zersponsex astege assodobim fan rkperate partionals petjram tiqualg the founds g\n",
      "================================================================================\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4100: 1.613526 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4200: 1.591262 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4300: 1.597753 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4400: 1.599454 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4500: 1.606749 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4600: 1.589981 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4700: 1.598190 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4800: 1.607431 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4900: 1.589249 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5000: 1.604977 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "rook can out the stace as the two gion histe part divic as quait corba relements\n",
      "ing the name with two zero four sedient interpreject constart small more extanno\n",
      "gnown iss extscty anjania of find to compessifles shimil withmallif two unded th\n",
      "ight the mihortaint person poty feights of rittribuain textruct a to this origin\n",
      "mera bagapist is a centpop year campis say is had to a vickalands music banks th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5100: 1.592999 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5200: 1.580970 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5300: 1.582636 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5400: 1.564502 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5500: 1.568069 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5600: 1.580046 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5700: 1.565004 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5800: 1.559679 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5900: 1.570781 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6000: 1.575102 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "================================================================================\n",
      "quing to cateer in difference and later human what polities that manigu mrmy myt\n",
      "jech put other by gorbell sttacol lomated in in cadiment by from had set of end \n",
      "us jamel this conticar one micks capportant from the rociable englip toratic de \n",
      " relacepian producer right long unit syrva m speral is the implements of hadia i\n",
      "x increase suark and alsially of aposta funterzocku rradaments subjam allomaty b\n",
      "================================================================================\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6100: 1.604486 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6200: 1.572475 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6300: 1.588794 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.99\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6400: 1.604568 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6500: 1.616812 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6600: 1.583834 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6700: 1.596159 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6800: 1.565346 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6900: 1.541230 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.15\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 7000: 1.586998 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "================================================================================\n",
      "ulthenally ost versing of that into known pake withmo on hisles of the paces of \n",
      "rivelar are sunflegus availantly gropes and seven zero zero zero one sig the exc\n",
      "ustrics of eqviding a trade and of gas then figned to classes deemons of duch op\n",
      "rived to mickes and the cotessed befanalar national nearly for the immore for ar\n",
      "ies him of yaily state to right an one eight seven ray as not of retais fidmente\n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n",
      "CPU times: user 5min 13s, sys: 35.4 s, total: 5min 48s\n",
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Parameters\n",
    "    ifcox = tf.Variable(tf.truncated_normal(\n",
    "        [vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "    ifcom = tf.Variable(tf.truncated_normal(\n",
    "        [num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal(\n",
    "        [num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        stuff = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "\n",
    "        input_gate = tf.sigmoid(stuff[:, :1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(stuff[:, 1 * num_nodes:2 * num_nodes])\n",
    "        output_gate = tf.sigmoid(stuff[:, 2 * num_nodes:3 * num_nodes])\n",
    "        update = stuff[:, 3 * num_nodes:]\n",
    "\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    # labels are inputs shifted by one time step.\n",
    "    train_labels = train_data[1:]\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few\n",
    "            # batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval(\n",
    "                            {sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplying futher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297306 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "acatezcm keiongkcupenyscefshno o ther houvbek iee wl ceoxpl temondou notnisbi ha\n",
      "a e  tliqrnxt rzsere ag pseofdbdqnynetasv pefbtbkzd gsgyyfivfwredtrfuyb yf yxqqe\n",
      "p tgonb xuam nhygc mdxurzutt  erkzscauatxqntyn grwajopzhjlrzwscpwdvelxmfiztvihtt\n",
      "h wrrxiswrray ai fhpgruytmpob  d  bso l liccgoit khvlpcfz  maovolgrtrorogiheiiho\n",
      "mfrtgd reeo er vw l xka dm eds httqqhy rilqvzjdrieerhrdense nrzokjtsjjkziv emimk\n",
      "================================================================================\n",
      "Validation set perplexity: 20.15\n",
      "Average loss at step 100: 2.610052 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.17\n",
      "Validation set perplexity: 10.51\n",
      "Average loss at step 200: 2.269278 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.81\n",
      "Validation set perplexity: 9.14\n",
      "Average loss at step 300: 2.122697 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 400: 2.027826 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.32\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 500: 1.949255 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 600: 1.888611 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.48\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 700: 1.876625 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 800: 1.854763 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 900: 1.823823 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1000: 1.813156 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "================================================================================\n",
      "ary the colduring airvondor airly dopecs hargingen premice therp the one wiorath\n",
      "henelal phite phunical host clatex as adjugateentainfs the are dilined anhee los\n",
      "ing in beg lines hore the eight rereventarly w incurection combnter in countrarm\n",
      "puct enair cat diribaker and menery chachers the sundubes whubor averpous figst \n",
      "tudy cauptions as all and kegational lent g d of metreen the cimmer his ulise or\n",
      "================================================================================\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1100: 1.798394 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1200: 1.757036 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 1300: 1.760472 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1400: 1.743019 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1500: 1.725296 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1600: 1.751455 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1700: 1.723756 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1800: 1.711043 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1900: 1.697434 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 2000: 1.703907 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "================================================================================\n",
      "c vilezition d liltiowall effecture mulion the prosimach finding to vipper inner\n",
      "oker obeaking vary of novered of the wil thence let foel then citce the gearce p\n",
      "fextel i roqumentation cental the enouriz i e symbon and relettly a disistal mal\n",
      "breti s the crepitbous of the church a courtatheconstandhs pition an somotions s\n",
      "x was many tfogned the tuch to pandacring perkirators four five five threen chan\n",
      "================================================================================\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 2100: 1.702185 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 2200: 1.689738 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 2300: 1.672714 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2400: 1.671244 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2500: 1.675409 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2600: 1.676969 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2700: 1.652615 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2800: 1.662925 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2900: 1.676309 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 3000: 1.627545 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "reands in the lack dm rana one eight zero a congaas lamong operation and bost co\n",
      "ty two see one one one zero comm one ennerate the the mann singatoties where a c\n",
      "ng u isciputs and identy his helmemates and dance the pareility are badgreatly p\n",
      "ns used partatch jupy the however four has also betwo eight six and actives conq\n",
      "fony nile general emien perams connume that seven mission ialy by demastern and \n",
      "================================================================================\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 3100: 1.623019 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3200: 1.634507 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3300: 1.648515 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 3400: 1.623891 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3500: 1.614894 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3600: 1.584437 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3700: 1.589054 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3800: 1.601281 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3900: 1.591402 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4000: 1.590576 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "================================================================================\n",
      "ber toky decont mix planese s assomadted botin experience and the bicliugive tho\n",
      "pridge newing audherial progreslen sib eight but of fahited formed esacifian cav\n",
      "y carkelim to into falmer which in less precal ten country boy collected to a mi\n",
      "que and mian were rmighous to and bebore gental shnotic depoctucus a smrike conc\n",
      "quire as yebria ty nebrical are bld termunitional raness or mimoria way dirf s t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4100: 1.553554 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4200: 1.578622 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4300: 1.586353 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4400: 1.558868 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4500: 1.555575 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4600: 1.566251 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4700: 1.577203 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4800: 1.585166 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4900: 1.598408 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5000: 1.562632 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "oldings into communian he lime to birted by the shere was eyonaly influence one \n",
      "ker the but any adoults herre of from chinker city of these quetorment in the mm\n",
      "qui s furte the scientwexistle of over it yleate a milor forum synorication to a\n",
      "chur kears disimed for that the or prackent intend that god to cauned a purcure \n",
      "ducum by typicning a macing murte base that examboran typered an abchazeroung of\n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5100: 1.554762 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5200: 1.549601 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5300: 1.573258 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5400: 1.569319 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5500: 1.547453 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5600: 1.582137 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5700: 1.572272 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5800: 1.580952 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5900: 1.571723 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6000: 1.582270 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "x controlftion his studie bur for the wewps a coust m g with around usedset this\n",
      "te for unstraber the god and pared by the fublars stacval of ievines with south \n",
      " its of bexation and zeocessal marits romapory but york city for maint be he sta\n",
      "y s just or throot adis hava were exkes adducted irah chroster in septical india\n",
      "ints and press abul freek americas was noites and india figh new shoit abrappi o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6100: 1.552820 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6200: 1.518358 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6300: 1.535354 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6400: 1.535704 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6500: 1.560378 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.76\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6600: 1.562762 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6700: 1.570408 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6800: 1.559567 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6900: 1.588577 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 7000: 1.567700 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "ces to only ameimction that held but structioned at incolication elace two zero \n",
      "y exclusional examplemorn actual ligitican epsomilly is hole barudal moved all m\n",
      "onitiss wough helds and luldle as grow rease had asvosopp the gauma notrics supp\n",
      "tern four play a minoriated that popularity the gull creation been mining actara\n",
      "geve to particiins and awscots after to migs the recriminal individurating as su\n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n",
      "CPU times: user 5min, sys: 28.3 s, total: 5min 28s\n",
      "Wall time: 3min 17s\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Parameters\n",
    "    ifcow = tf.Variable(tf.truncated_normal(\n",
    "        [vocabulary_size + num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal(\n",
    "        [num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "\n",
    "        stuff = tf.matmul(tf.concat(1, [i, o]), ifcow) + ifcob\n",
    "\n",
    "        input_gate = tf.sigmoid(stuff[:, :1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(stuff[:, 1 * num_nodes:2 * num_nodes])\n",
    "        output_gate = tf.sigmoid(stuff[:, 2 * num_nodes:3 * num_nodes])\n",
    "        update = stuff[:, 3 * num_nodes:]\n",
    "\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    # labels are inputs shifted by one time step.\n",
    "    train_labels = train_data[1:]\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few\n",
    "            # batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval(\n",
    "                            {sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach by [Sujit Pal](https://discussions.udacity.com/t/assignment-6-problem-1-model-not-working-after-increased-dimensions/157224/2?u=brtknr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 3.29828453064 learning rate: 10.0\n",
      "Minibatch perplexity: 27.07\n",
      "================================================================================\n",
      "mlcigamnrqieoon n czahte o lezcb wsiesyolcol ka zn hr bw  d lmtmv pbejoesfsrl qe\n",
      "xnenztw ernh s jyt gn nlzaentaij  rejj pkoylocbhiorjaoju fhsac zhc h sk epw q ay\n",
      "cvibjturyyonzf ovdqk qrbtio s iesi  oi ppidyabo enitac a e axubencejilz  pfdk is\n",
      "eo  b br sisou nafar troat  p  f on imesp hqr tudbrgpheb uycl io  oiprv i gxf te\n",
      "dvepft u eee b  tnqjch abvddz ir zttqw ia gnrst  k xguef yetn  peri   wieg sc ns\n",
      "================================================================================\n",
      "Validation set perplexity: 21.14\n",
      "Average loss at step 100 : 2.55518705606 learning rate: 10.0\n",
      "Minibatch perplexity: 10.19\n",
      "Validation set perplexity: 10.12\n",
      "Average loss at step 200 : 2.22373357773 learning rate: 10.0\n",
      "Minibatch perplexity: 8.41\n",
      "Validation set perplexity: 8.77\n",
      "Average loss at step 300 : 2.10940693736 learning rate: 10.0\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 400 : 2.06510280609 learning rate: 10.0\n",
      "Minibatch perplexity: 8.19\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 500 : 2.03542437792 learning rate: 10.0\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 600 : 1.97017789721 learning rate: 10.0\n",
      "Minibatch perplexity: 7.15\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 700 : 1.95698185444 learning rate: 10.0\n",
      "Minibatch perplexity: 7.64\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 800 : 1.96499086261 learning rate: 10.0\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 900 : 1.95330669045 learning rate: 10.0\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 1000 : 1.95321278691 learning rate: 10.0\n",
      "Minibatch perplexity: 7.44\n",
      "================================================================================\n",
      "uturancs reureally usive the one seven one five onitical senthest and rountropir\n",
      "munder kane fivent her grow an resercuss is heluwed pe cory gho m simeds shous a\n",
      "olopdise d dotamer fronot packarus carge frobis is o yeric hordernimonic anoly l\n",
      "ropih goulyy thlast devely dor and is sens ruck one amissuet wasonary svirder to\n",
      "ome hanculivit witisu anersit h devenor plense mougome b bavionistions thrim sev\n",
      "================================================================================\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 1100 : 1.91916403294 learning rate: 10.0\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 1200 : 1.90197667837 learning rate: 10.0\n",
      "Minibatch perplexity: 7.37\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 1300 : 1.89568286896 learning rate: 10.0\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 1400 : 1.90133735776 learning rate: 10.0\n",
      "Minibatch perplexity: 7.29\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 1500 : 1.89648487449 learning rate: 10.0\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 1600 : 1.8787385428 learning rate: 10.0\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 1700 : 1.87124349833 learning rate: 10.0\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 1800 : 1.84721987963 learning rate: 10.0\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 1900 : 1.85065302134 learning rate: 10.0\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 2000 : 1.84702714443 learning rate: 10.0\n",
      "Minibatch perplexity: 5.92\n",
      "================================================================================\n",
      "ress the amad nabhh to gogher an or papave the mestractand coup jerpethe rich ma\n",
      "jlest luref s the pusts tolles ple purary the eight thout of pevenatus eceatly a\n",
      "ribany sear swechftern ors were as his the instratoms and lemeno your his facced\n",
      "ued currizanu resurb cootrar derments pretive two zeree of methets intreges hest\n",
      "qued strace fa cans stroffenian scacatantarizas the informes femenbrew all ruing\n",
      "================================================================================\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 2100 : 1.85988706231 learning rate: 10.0\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 2200 : 1.88247145534 learning rate: 10.0\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 2300 : 1.88982655287 learning rate: 10.0\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 2400 : 1.8610686779 learning rate: 10.0\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 2500 : 1.86664460301 learning rate: 10.0\n",
      "Minibatch perplexity: 7.34\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 2600 : 1.8578401494 learning rate: 10.0\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 2700 : 1.87230358005 learning rate: 10.0\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 2800 : 1.8623995316 learning rate: 10.0\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 2900 : 1.86753990412 learning rate: 10.0\n",
      "Minibatch perplexity: 7.69\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 3000 : 1.86482348561 learning rate: 10.0\n",
      "Minibatch perplexity: 5.96\n",
      "================================================================================\n",
      "pats own sevich is herners dism beatoler the rome the mom renay is lamaite unter\n",
      "d proving the this sining of the sqwe thisoan cat deension secrated free dirorti\n",
      "x pes this on ttrict malane over re lark is biffered seeks but the engorm one tt\n",
      "mon canbre pofiel thurms gork the crence tares of the gealan zerolug the intend \n",
      "p indint thes m of the vichu bbrop frentem welent ost and mock acting amavions s\n",
      "================================================================================\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 3100 : 1.84072400689 learning rate: 10.0\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 3200 : 1.82039517641 learning rate: 10.0\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 3300 : 1.83968978405 learning rate: 10.0\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 3400 : 1.82246793628 learning rate: 10.0\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 3500 : 1.86689637899 learning rate: 10.0\n",
      "Minibatch perplexity: 7.17\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 3600 : 1.84075977206 learning rate: 10.0\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 3700 : 1.84679960251 learning rate: 10.0\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 3800 : 1.85169780135 learning rate: 10.0\n",
      "Minibatch perplexity: 6.96\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 3900 : 1.85318265319 learning rate: 10.0\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 4000 : 1.84076391816 learning rate: 10.0\n",
      "Minibatch perplexity: 6.45\n",
      "================================================================================\n",
      "dons a was an neal inco grouncienkiced signan whercied and jecilodners trays mak\n",
      "mensazy such in wsslanked whoaders aner milk fating is an helrenters the nows s \n",
      "x and bbrineldure engerventats was polserttres unibe these dode they gibplente n\n",
      "parted as akaot worerate adocis the duti dod os is in pombooving bria fay for ge\n",
      "que while damested smal one thre ftral avaliary kocked to fusort seacections whe\n",
      "================================================================================\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 4100 : 1.81547446966 learning rate: 10.0\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 4200 : 1.81170496106 learning rate: 10.0\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 4300 : 1.81409865737 learning rate: 10.0\n",
      "Minibatch perplexity: 6.70\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 4400 : 1.8031149888 learning rate: 10.0\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 4500 : 1.83464768648 learning rate: 10.0\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 4600 : 1.83003341556 learning rate: 10.0\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 4700 : 1.82889019251 learning rate: 10.0\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 4800 : 1.81546188116 learning rate: 10.0\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 4900 : 1.82122421861 learning rate: 10.0\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 5000 : 1.81571546793 learning rate: 1.0\n",
      "Minibatch perplexity: 6.12\n",
      "================================================================================\n",
      "les usy sentete minne nate one two spad scale and stamnce and alafoseer the nunc\n",
      "ing web the at charly one tine are increded kmope pris in develomatiorsormaniste\n",
      "t of to nglmenceds or of the aftretfing the flies cond bille europance as an beo\n",
      "lanatus ted as by it even ming three eight six zero canm or ams lepies is metrad\n",
      "naratic neps dunumicaget mansudace ailmer arhic beop areiease gradrom vatle to f\n",
      "================================================================================\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 5100 : 1.77952620864 learning rate: 1.0\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 5200 : 1.7731631875 learning rate: 1.0\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 5300 : 1.77546009898 learning rate: 1.0\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 5400 : 1.77462376475 learning rate: 1.0\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 5500 : 1.77247163534 learning rate: 1.0\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 5600 : 1.74103203416 learning rate: 1.0\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 5700 : 1.74644698977 learning rate: 1.0\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 5800 : 1.76888860941 learning rate: 1.0\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 5900 : 1.75025490642 learning rate: 1.0\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 6000 : 1.74975926757 learning rate: 1.0\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "le eld bekbrites is encleching nead one north estephy first astrod manure they b\n",
      "xion and ttrional bamingic had anverce war the b rea of mmusely in and seciatant\n",
      "ound ice her prosm as attles as the a sorvent mecephy seputed image host starly \n",
      "leentic of is worlds odeves thas including yanure and mansea joyrap l the the pr\n",
      "x of differse linether ttriates eab bunets the frearly that for lonnuaiy one eig\n",
      "================================================================================\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 6100 : 1.74260700703 learning rate: 1.0\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 6200 : 1.75369041562 learning rate: 1.0\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 6300 : 1.75361113548 learning rate: 1.0\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 6400 : 1.74562925577 learning rate: 1.0\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 6500 : 1.72607207179 learning rate: 1.0\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 6600 : 1.77531499982 learning rate: 1.0\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 6700 : 1.74718939424 learning rate: 1.0\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 6800 : 1.74771927238 learning rate: 1.0\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 6900 : 1.74701392889 learning rate: 1.0\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 7000 : 1.75858810902 learning rate: 1.0\n",
      "Minibatch perplexity: 5.74\n",
      "================================================================================\n",
      "bjer and pester sood the peavitation a they ondicl sensed by which the bertall n\n",
      "y valtion condics they by be after fobocks atcher the croming in one nine zero z\n",
      "tice to turn the also to impard agen the fingina and and witherod tt haysed also\n",
      "stely one sinet multratiun leandanion on ty parting on tinchtred celtications br\n",
      "jutor frain in eighorer signizars monal have actern sannading the eation hance a\n",
      "================================================================================\n",
      "Validation set perplexity: 5.72\n",
      "CPU times: user 5min 49s, sys: 38.8 s, total: 6min 28s\n",
      "Wall time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    wx = tf.Variable(tf.truncated_normal([4*vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    wm = tf.Variable(tf.truncated_normal([4*num_nodes, num_nodes], -0.1, 0.1))\n",
    "    \n",
    "    # Input gate: input, previous output, and bias.\n",
    "#     ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "#     im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "#     fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "#     fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "#     cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "#     cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "#     ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "#     om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        i_stacked = tf.concat(1, [i, i, i, i])\n",
    "        o_stacked = tf.concat(1, [o, o, o, o])\n",
    "        weights_in = tf.matmul(i_stacked, wx)\n",
    "        weights_out = tf.matmul(o_stacked, wm)        \n",
    "#         input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "#         forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "#         update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        input_gate = tf.sigmoid(weights_in + weights_out + ib)\n",
    "        forget_gate = tf.sigmoid(weights_in + weights_out + fb)\n",
    "        update = weights_in + weights_out + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "#         output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        output_gate = tf.sigmoid(weights_in + weights_out + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in xrange(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print ('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in xrange(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print ('Average loss at step', step, ':', mean_loss, 'learning rate:', lr)\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print ('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print ('=' * 80)\n",
    "                for _ in xrange(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in xrange(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print (sentence)\n",
    "                print ('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in xrange(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print ('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(\n",
    "            shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic instituti'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
